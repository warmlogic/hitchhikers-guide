{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Text Analysis is used for summarizing or getting useful information out of a large amount of unstructured text stored in documents. This opens up the opportunity of using text data alongside more conventional data sources (e.g, surveys and administrative data). The goal of text analysis is to take a large corpus of complex and unstructured text data and extract important and meaningful messages in a comprehensible meaningful way. \n",
    "\n",
    "\n",
    "Text Analysis can help with the following tasks:\n",
    "\n",
    "* **Informationan retrieval**: Help find relevant information in large databases such as a systematic literature review. \n",
    "\n",
    "* **Clustering and text categorization**: Techniques like topic modeling can summarize a large corpus of text by finding the most important phrases. \n",
    "\n",
    "* **Text Summarization**: Create category-sensitive text summaries of a large corpus of text. \n",
    "\n",
    "* **Machine Translation**: Translate from one language to another. \n",
    "\n",
    "In this tutorial, we are going to analyze job advertisements from 2010-2015 using topic modeling to examine the content of our data and document classification to tag the type of job in the advertisement. First we will go over how to transform our data into a matrix that can be read in by an algorithm. \n",
    " \n",
    "\n",
    "\n",
    "## Glossary of Terms\n",
    "\n",
    "* **Corpus**: A corpus of documents is the set of all documents in the dataset.\n",
    "\n",
    "* **Tokenize**: Tokenization is the process by which text is sepearated into meaningful terms or phrases. In english this is fairly triial as words as separated by whitespace. \n",
    "\n",
    "* **Stemming**: Stemming is a type of text normalization where words that have different forms but their essential meaning at the same are normalized to the original dictionary form of a word. For example \"go,\" \"went,\" and \"goes\" all stem from the lemma \"go.\"\n",
    "\n",
    "* **TFIDF**: TFIDF (Term frequency-inverse document frequency) is an example of feature enginnering where the most important words are extracted by taking account their frequency in documents and the entire corpus of documents as a whole.\n",
    "\n",
    "* **Topic Modeling**: Topic modeling is an unsupervised learning method where groups of co-occuring words are clustered into topics. Typically, the words in a cluster should be related and make sense (e.g, boat, ship, captain). Individual documents will then fall into multiple topics. \n",
    "\n",
    "* **LDA**: LDA (latent Dirichlet allocation) is a type of probabilistic model commonly used for topic modelling. \n",
    "\n",
    "* **Stop Words**: Stop words are words that have little semantic meaning like prepositions, articles and common nouns. They can often be ignored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline \n",
    "import nltk\n",
    "import ujson\n",
    "import re\n",
    "import time\n",
    "import progressbar\n",
    "\n",
    "import pandas as pd\n",
    "from __future__ import print_function\n",
    "from six.moves import zip, range \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, auc\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter, OrderedDict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords') #download the latest stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "\n",
    "Our Dataset for this tutorial will be a subset jobs-ad data from 2010-2015 compiled by the Commonwealth of Virginia. The full data and how this subset was created can be found in the data folder in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs_data = pd.read_csv('./data/jobs_subset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our table has 4 fields. `normalizedTitle_onetName`, `normalizedTitle_onetCode`, `jobDescription`, `title`\n",
    "\n",
    "Onet is an online database that contains hundreds of occupational definitions. https://en.wikipedia.org/wiki/Occupational_Information_Network\n",
    "\n",
    "The normalizedTitle_onetName and the normalizedTitle_onetCode are derived from the Onet Database. We wil use the names in the document tagging portion of the tutorial. The jobDescription is the actual jobDescription and the title is derived from the jobDescription. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many unique job titles are in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs_data.normalizedTitle_onetName.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs_data.title.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs_data.title.unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5 unique categories of jobs using the ONet classification. There are too many unique job titles in the title field to display. We can see the shape of the array of unique titles is 2496 titles. \n",
    "\n",
    "Each job description has a great deal of information contained in unstructered text. We can use text analysis to find overarching concepts that are in our corpus.  This will allow us to the discover the most important words and phrases in the job descriptions and give us a big-picture of the content in our collection. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    " We are going to apply topic modeling, an unsuperivised learning method, to our corpus to find the high-level topics in our corpus as a first-go for exploring our data. As we apply topic modeling we will discuss ways of cleaning and preprocessing our data to get the best results.\n",
    "\n",
    "Topic modeling is a broad subfield of machine learning and natural language processing. We are going to focus on one approach, Latent Dirichlet allocation (LDA). LDA is a fully Bayesian extension of probabilistic latent semantic  indexing, itself a probabilistic extension of latent semantic analysis.\n",
    "\n",
    "In topic modeling we first assume the existence of topics in the corpus and that there is a small number of topics that can explain a corpus. Topics, in this case, are a ranked-list of words from our corpus, with the highest probability words at the top. A single document can be explained by multiple topics. For instance, an article on net neutrality has to do with both technology and politics. The set of topics used by a document is known as the document's allocation, hence, the name latent Dirchlet allocation, each document has an allocation of latent topics allocated by Dirchlet distribution. \n",
    "\n",
    "## Processing text data\n",
    "\n",
    "The first important step in working with text data is cleaning and processing the data. This include but is not limited to *forming a corpus of text, tokenization, removing stop-words, finding words colocated together (N-grams), and stemming and lemmatization*. Each of these steps will be discussed below. The ultimate goal is to transform our text data into a form an algorithm can work with. A sequence of symbols cannot be fed directly into an algorithm. Algorithms expect numerical feature vectors with fixed size rather then a document with a variable document length. We will be transforming our text corpus into a *bag of n-grams* to be further analyzed. In this form our text data is represented as a matrix where each row refers to a specific job description (document) and each column is the occurence of a word (feature).\n",
    "\n",
    "\n",
    "### Bag of n-gram representation example\n",
    "\n",
    "Ultimately, we want to take our collection of documents, corpus, and convert it into a matrix. Fortunately sklearn has a pre-built object, `CountVectorizer`, that can tokenize, eliminate stopwords, identify n-grams and stem our corpus, outputing a matrix in one step. Before we apply the vectorizer on our corpus of data we are going to apply it to a toy example so we can understand the output and how a bag of words is represented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_words(corpus,\n",
    "                       NGRAM_RANGE=(0,1),\n",
    "                       stop_words = None,\n",
    "                        stem = False,\n",
    "                       MIN_DF = 0.05,\n",
    "                       MAX_DF = 0.95,\n",
    "                       USE_IDF=False):\n",
    "    \"\"\"\n",
    "    Turn a corpus of text into a bag-of-words.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    corpus: ls\n",
    "        test of documents in corpus    \n",
    "    NGRAM_RANGE: tupule\n",
    "        range of N-gram default (0,1)\n",
    "    stop_words: ls\n",
    "        list of commonly occuring words that have little semantic\n",
    "        value\n",
    "    stem: bool\n",
    "        use a stemmer to stem words\n",
    "    MIN_DF: float\n",
    "       exclude words that have a frequency less than the threshold\n",
    "    MAX_DF: float\n",
    "        exclude words that have a frequency greater than the threshold\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bag_of_words: scipy sparse matrix\n",
    "        scipy sparse matrix of text\n",
    "    features:\n",
    "        ls of words\n",
    "    \"\"\"\n",
    "    #parameters for vectorizer \n",
    "    ANALYZER = \"word\" #unit of features are single words rather then phrases of words \n",
    "    STRIP_ACCENTS = 'unicode'\n",
    "     \n",
    "    if stem:\n",
    "        tokenize = lambda x: [stemmer.stem(i) for i in x.split()]\n",
    "    else:\n",
    "        tokenize = None\n",
    "    vectorizer = CountVectorizer(analyzer=ANALYZER,\n",
    "                                tokenizer=tokenize, \n",
    "                                ngram_range=NGRAM_RANGE,\n",
    "                                stop_words = stop_words,\n",
    "                                strip_accents=STRIP_ACCENTS,\n",
    "                                min_df = MIN_DF,\n",
    "                                max_df = MAX_DF)\n",
    "    \n",
    "    bag_of_words = vectorizer.fit_transform( corpus ) #transform our corpus is a bag of words \n",
    "    features = vectorizer.get_feature_names()\n",
    "\n",
    "    if USE_IDF:\n",
    "        NORM = None #turn on normalization flag\n",
    "        SMOOTH_IDF = True #prvents division by zero errors\n",
    "        SUBLINEAR_IDF = True #replace TF with 1 + log(TF)\n",
    "        transformer = TfidfTransformer(norm = NORM,smooth_idf = SMOOTH_IDF,sublinear_tf = True)\n",
    "        #get the bag-of-words from the vectorizer and\n",
    "        #then use TFIDF to limit the tokens found throughout the text \n",
    "        tfidf = transformer.fit_transform(bag_of_words)\n",
    "        \n",
    "        return tfidf, features\n",
    "    else:\n",
    "        return bag_of_words, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_corpus = ['this is document one', 'this is document two', 'text analysis on documents is fun'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_bag_of_words, toy_features = create_bag_of_words(toy_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The counter_vectorizer outputs a matrix. In this case a sparse matrix, a matrix that has a lot more 0s then 1s. To save space scipy has special methods for storing sparse matrices in a space-efficient way rather than saving many many 0s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_bag_of_words = toy_bag_of_words.toarray()\n",
    "np_bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data has been transformed into a 3x9 matrix where each row corresponds to a document and the columns correspond to the features. A 1 indicates the existence of the feature or word in the document, 0 indicates the word is not present.Our toy corpus is now ready to be analyzed. We illustrated bag of n-gram with a toy example because the matrix for a much larger corpus would be much larger and harder to interpet on our corpus of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### word counts\n",
    "\n",
    "As a initial look into the data we can examine what the top few words are in our corpus. We can sum the columns of the bag_of_words and then convert to a numpy array. From here we can zip the features and word_count into a dictionary\n",
    "and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_counts(bag_of_words, feature_names):\n",
    "    \"\"\"\n",
    "    Get the ordered word counts from a bag_of_words\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bag_of_words: obj\n",
    "        scipy sparse matrix from CounterVectorizer\n",
    "    feature_names: ls\n",
    "        list of words\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    word_counts: dict\n",
    "        Dictionary of word counts\n",
    "    \"\"\"\n",
    "    np_bag_of_words = bag_of_words.toarray()\n",
    "    word_count = np.sum(np_bag_of_words,axis=0)\n",
    "    np_word_count = np.asarray(word_count).ravel()\n",
    "    dict_word_counts = dict( zip(feature_names, np_word_count) )\n",
    "    \n",
    "    orddict_word_counts = OrderedDict( \n",
    "                                    sorted(dict_word_counts.items(), key=lambda x: x[1], reverse=True), )\n",
    "    \n",
    "    return orddict_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_word_counts(toy_bag_of_words, toy_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Corpora\n",
    "\n",
    "First we need to form our corpus, a set of multiple similiar documents. In our case, our corpus is the set of all job descriptions. We can pull out the job descriptions from the data frame by pulling out the underlying numpy array using the `.values` attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df_jobs_data['jobDescription'].values #pull all the jobDescriptions and put them in a numpy array \n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topics(tfidf, features, N_TOPICS=3, N_TOP_WORDS=5,):\n",
    "    \"\"\"\n",
    "    Given a matrix of features of text data generate topics\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    tfidf: scipy sparse matrix\n",
    "        sparse matrix of text features\n",
    "    N_TOPICS: int\n",
    "        number of topics (default 10)\n",
    "    N_TOP_WORDS: int\n",
    "        number of top words to display in each topic (default 10)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    ls_keywords: ls\n",
    "        list of keywords for each topics\n",
    "    doctopic: array\n",
    "        numpy array with percentages of topic that fit each category\n",
    "    N_TOPICS: int\n",
    "        number of assumed topics\n",
    "    N_TOP_WORDS: int\n",
    "        Number of top words in a given topic. \n",
    "    \"\"\"\n",
    "    \n",
    "    with progressbar.ProgressBar(max_value=progressbar.UnknownLength) as bar:\n",
    "        i=0\n",
    "        lda = LatentDirichletAllocation( n_topics= N_TOPICS,\n",
    "                                       learning_method='online') #create an object that will create 5 topics\n",
    "        bar.update(i)\n",
    "        i+=1\n",
    "        doctopic = lda.fit_transform( tfidf )\n",
    "        bar.update(i)\n",
    "        i+=1\n",
    "        \n",
    "        ls_keywords = []\n",
    "        for i,topic in enumerate(lda.components_):\n",
    "            word_idx = np.argsort(topic)[::-1][:N_TOP_WORDS]\n",
    "            keywords = ', '.join( features[i] for i in word_idx)\n",
    "            ls_keywords.append(keywords)\n",
    "            print(i, keywords)\n",
    "            bar.update(i)\n",
    "            i+=1\n",
    "            \n",
    "    return ls_keywords, doctopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_bag_of_words, corpus_features = create_bag_of_words(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine our features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first aspect of the feature list that should stand out for us is that the first few entries are numbers that have no real semantic meaning. There are also other useless words such as prepositions and articles that also have no semantic meaning. The words *ability* or *abilities* or *accuracy* and *accurate* are also quite similiar and mean the same thing. We should try cleaning our corpus of data of these types of words as they just add noise to our analysis. Nevertheless let's try creating topics and seeing the quality of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_word_counts(corpus_bag_of_words, corpus_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our top words are articles, prepositions and conjunctions that do not tell us anything about our courpus. Let's march on create topics anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_corpus_keywords, corpus_doctopic = create_topics(corpus_bag_of_words, corpus_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these topics we have no real knowledge of what is in our corpus, with the exception that there are job ads written in Spanish. The problem is the the top words in the topics are conjunctions and prepositions that have no semantic information. We have to try and clean and process our data to get more meaningful infomation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning and Normalization\n",
    "\n",
    "To clean and normalize text we will remove all special characters, numbers, and punctuation. Then we will make all the text lowercase to normalize the text;  this is so words like \"the\" and \"The\" will be counted as the same in our analysis. To remove the special characters, numbers and punctuation we will use regular expressions. \n",
    "\n",
    "#### Regular Expressions\n",
    "\n",
    ">\"Some people, when confronted with a problem, think \n",
    ">'I know, I'll use regular expressions.'   Now they have two problems.\"\n",
    "> -- Jaime Zawinski\n",
    "\n",
    "Regular Expressions or regexes match a certain amount text in a document based on a set of rules and syntax. The name \"regular expressions\" actually comes from the mathematical theory it is based on. These rules are useful for pulling out useful information in a large amount of text (e.g., email addresses, html-tags, credit card numbers). Regexes often match text much more quickly then plain text sorting and can often reduce their development time. Some regular expressions can become quite complicated and it may then become a good idea to write code using Python. Any developer should keep in mind there is a trade-off between optimization and understandibility. In Python, a general philosophy is code is meant to be as understandable by *people* as much as possible. You should therefore always tend toward the understandabilty side of things rather than overly optimizing your code. Your future-self, code-reviewers, people who inherit your code, and anyone else who has to make sense of your code in the future will appreciate it. \n",
    "\n",
    "For our purposes we are going to use a regular expression to match all characters that are not letters -- punctutation, quotes, special characters and numbers --replace them with spaces and then take all the remaining characters and make them lowercase. \n",
    "\n",
    "A full tutorial on regular expressions would be outside the scope of this tutorial. There are many good tutorials that can be found on-line. There is also a great interactive tool for developing and checking regular expressions regex101.com. \n",
    "\n",
    "We will be using the `re` library in python for regular expression matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of the punctuations and set all characters to lowercase\n",
    "RE_PREPROCESS = r'\\W+|\\d+' #the regular expressions that matches all non-characters\n",
    "\n",
    "#get rid of punctuation and make everything lowercase\n",
    "#the code belows works by looping through the array of text\n",
    "#for a given piece of text we invoke the `re.sub` command where we pass in the regular expression, a space ' ' to\n",
    "#subsitute all the matching characters with\n",
    "#we then invoke the `lower()` method on the output of the re.sub command\n",
    "#to make all the remaining characters\n",
    "#the cleaned document is then stored in a list\n",
    "#once this list has been filed it is then stored in a numpy array\n",
    "\n",
    "processed_corpus = np.array( [ re.sub(RE_PREPROCESS, ' ', comment).lower() for comment in corpus] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first decription before cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first description after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All lowercase, all numbers and special chracters have been removed. Out text is now normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Now that we have cleaned our text we can tokenize it by deciding which terms and phrases are the most meaningful. In this case we want to split our text into individual words. Our words are separted by spaces so we can use the `.split()` command to turn are document into a list of words using a space as the character to split on as an example. Normally the `CountVectorizer` handles this for us.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = processed_corpus[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "Stopwords are words that have very little semantic meaning and are found throughout a text. Having the word *the* or *of* will tell us nothing about our corpus, nor will they be meaningful features.  Examples of stopwords are prepositions, articles and common nouns. To process the corpus even further we can eliminate these stopwords by checking if the are in a list of commonly occuring stopwords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords =  stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample of stopwords\n",
    "eng_stopwords[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_bag_of_words, processed_features = create_bag_of_words(processed_corpus,stop_words=eng_stopwords)\n",
    "dict_processed_word_counts = get_word_counts(processed_bag_of_words, processed_features)\n",
    "dict_processed_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! Now let's see how this affects the topics that are produce. Though the top 20 words are likely to be in all the job ads. Let's add them to the stopwords to remove them as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_words = list(dict_processed_word_counts.keys())[:20]\n",
    "domain_specific_stopwords = eng_stopwords + top_20_words\n",
    "processed_bag_of_words, processed_features = create_bag_of_words(processed_corpus,\n",
    "                                                                 stop_words=domain_specific_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_processed_word_counts = get_word_counts(processed_bag_of_words, processed_features)\n",
    "dict_processed_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bit better. Let's see what topics we produce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_keywords, processed_doctopic = create_topics(processed_bag_of_words, \n",
    "                                                       processed_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are starting to get somewhere! There are a lot of jobs that have to do with the law and engineering and medicine. We should increase the number of topics and words for each topics to see if we can understand more from our corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_keywords, processed_doctopic = create_topics(processed_bag_of_words, \n",
    "                                                       processed_features, \n",
    "                                                      N_TOPICS = 5,\n",
    "                                                      N_TOP_WORDS= 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding more topics has revealed to larger subtopics. Let's see if adding 10 topics will tell us more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_keywords, processed_doctopic = create_topics(processed_bag_of_words, \n",
    "                                                       processed_features, \n",
    "                                                      N_TOPICS = 10,\n",
    "                                                      N_TOP_WORDS= 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have a good amount of topics. Some of the top words are quite similiar such as engineering and enginner. We can reduce those words to its stem to further refine our features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and lemmitzation\n",
    "\n",
    "We can further process our text through *stemming and lemmatization*. Words can take on muliple forms with limited change to their meaning. For example \"systems\", \"systematic\" and \"system\" are all different words but they all have the same meeting. We can replace all these words with system witout losing any meaning. The lemma is the original dictionary form of a word (e.g. lying and lie). There are several well known stemming algorithms -- Porter, Snowball, Lancaster -- that all have strengths and weakneses. For this tutorial we are using the Porter Stemmer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('lies'))\n",
    "print(stemmer.stem(\"lying\"))\n",
    "print(stemmer.stem('systematic'))\n",
    "print(stemmer.stem(\"running\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_bag_of_words, processed_features = create_bag_of_words(processed_corpus,\n",
    "                                                                 stop_words=domain_specific_stopwords,\n",
    "                                                                 stem=True)\n",
    "processed_keywords, processed_doctopic = create_topics(processed_bag_of_words, \n",
    "                                                       processed_features, \n",
    "                                                      N_TOPICS = 10,\n",
    "                                                      N_TOP_WORDS= 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not it appears we have picked up some extra topics that describe the educational requirements for a job ad or the equal opportunity clause of a job ad. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-grams\n",
    "\n",
    "Individual words are not always the the correct unit of analysis. Prematurely removing stopwords can lead to losing pharases such as \"kick the bucket\", \"commander in chief\", or \"sleeps with the fishes\". Idenitfying these N-grams requires looking for patterns of words that often appear together in fixed patterns. \n",
    "\n",
    "Now let's transform our corpus into a bag of n-grams which in this case is a bag of bi-grams or bag of 2-grams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_bag_of_words, processed_features = create_bag_of_words(processed_corpus,\n",
    "                                                                 stop_words=domain_specific_stopwords,\n",
    "                                                                 stem=True,\n",
    "                                                                 NGRAM_RANGE=(0,2))\n",
    "processed_keywords, processed_doctopic = create_topics(processed_bag_of_words, \n",
    "                                                       processed_features, \n",
    "                                                      N_TOPICS = 10,\n",
    "                                                      N_TOP_WORDS= 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice one of the top words in one of the topics is \"northrop grumman\", a bi-gram!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF (Term Frequency Inverse Document Frequency)\n",
    "\n",
    "A final step in cleaning and processing our text data is TFIDF. TFIDF (Term frequency-inverse document frequency) is an example of feature enginnering where the most important words are extracted by taking account their frequency in documents and the entire corpus of documents as a whole. Words that appear in all documents are deemphazized while more meaningful words are emphaized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_bag_of_words, processed_features = create_bag_of_words(processed_corpus,\n",
    "                                                                 stop_words=domain_specific_stopwords,\n",
    "                                                                 stem=True,\n",
    "                                                                 NGRAM_RANGE=(0,2),\n",
    "                                                                 USE_IDF = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_word_counts = get_word_counts(processed_bag_of_words,\n",
    "                   processed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words counts have been reweighted to emphasize the more meaninful words of the corpus while deemphasizing the the are found throughout the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_keywords, processed_doctopic = create_topics(processed_bag_of_words, \n",
    "                                                       processed_features, \n",
    "                                                      N_TOPICS = 10,\n",
    "                                                      N_TOP_WORDS= 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab the topic_id of the majority topic for each document and store it in a list\n",
    "ls_topic_id = [np.argsort(processed_doctopic[comment_id])[::-1][0] for comment_id in range(len(corpus))]\n",
    "df_jobs_data['topic_id'] = ls_topic_id #add to the dataframe so we can compare with the job titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that each row is tagged with a topic id let's see how well the topics exaplain the job advertisements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_num = 0\n",
    "print(processed_keywords[topic_num])\n",
    "df_jobs_data[ df_jobs_data.topic_id == topic_num ].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning: Document Classification.\n",
    "\n",
    "Now we turn our attention to supervised learning. Previously, using topic modelling, we were inferring relationships between the data. In supervised learning, we produce a label, *y*, given some data *x*. In order to produce labels we need to first have examples our algorithm can learn from, a training set. Developing a training set can be very expensive, as it can require a large amount of human labor or linguistic expertise. Document classification is the case where our x are documents and our y are what the documents are (e.g, title for a job position). A common example of document classification is spam detection in emails. In sentiment analysis our x is our documents and y is the state of the author. This can range from an author being happy or unhappy with a product or the author being pollitically conservative or liberal. There is also Part-of-speech tagging were our x are individual words and y is the part-of-speech. \n",
    "\n",
    "In this section we are going to train a classifier to classify job titles using our jobs dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./data/train_corpus_document_tagging.csv')\n",
    "df_test = pd.read_csv('./data/test_corpus_document_tagging.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['normalizedTitle_onetName'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(df_train['normalizedTitle_onetName'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['normalizedTitle_onetName'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(df_test['normalizedTitle_onetName'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is job advertisements for credit analysts and financial examiners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process our Data\n",
    "\n",
    "In order to feed out data into a classifier we need to pull out the labels, our y's, and a clean corpus of documents, x, for our training and testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = df_train.normalizedTitle_onetName.values\n",
    "train_corpus = np.array( [re.sub(RE_PREPROCESS, ' ', text).lower() for text in df_train.jobDescription.values])\n",
    "test_labels = df_test.normalizedTitle_onetName.values\n",
    "test_corpus = np.array( [re.sub(RE_PREPROCESS, ' ', text).lower() for text in df_test.jobDescription.values])\n",
    "labels = np.append(train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we had done in the unsupervised learning we have to transform our data. This time we have to transform our testing and training set into two different bag-of-words. The classifer will learn from the training set and we will evaluate the clasffier's performance on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for vectorizer \n",
    "ANALYZER = \"word\" #unit of features are single words rather then phrases of words \n",
    "STRIP_ACCENTS = 'unicode'\n",
    "TOKENIZER = None\n",
    "NGRAM_RANGE = (0,2) #Range for pharases of words\n",
    "MIN_DF = 0.01 # Exclude words that have a frequency less than the threshold\n",
    "MAX_DF = 0.8  # Exclude words that have a frequency greater then the threshold \n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=ANALYZER,\n",
    "                            tokenizer=None, # alternatively tokenize_and_stem but it will be slower \n",
    "                            ngram_range=NGRAM_RANGE,\n",
    "                            stop_words = stopwords.words('english'),\n",
    "                            strip_accents=STRIP_ACCENTS,\n",
    "                            min_df = MIN_DF,\n",
    "                            max_df = MAX_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORM = None #turn on normalization flag\n",
    "SMOOTH_IDF = True #prvents division by zero errors\n",
    "SUBLINEAR_IDF = True #replace TF with 1 + log(TF)\n",
    "USE_IDF = True #flag to control whether to use TFIDF\n",
    "\n",
    "transformer = TfidfTransformer(norm = NORM,smooth_idf = SMOOTH_IDF,sublinear_tf = True)\n",
    "\n",
    "#get the bag-of-words from the vectorizer and\n",
    "#then use TFIDF to limit the tokens found throughout the text \n",
    "start_time = time.time()\n",
    "train_bag_of_words = vectorizer.fit_transform( train_corpus ) #using all the data on for generating features!! Bad!\n",
    "test_bag_of_words = vectorizer.transform( test_corpus )\n",
    "if USE_IDF:\n",
    "    train_tfidf = transformer.fit_transform(train_bag_of_words)\n",
    "    test_tfidf = transformer.transform(test_bag_of_words)\n",
    "features = vectorizer.get_feature_names()\n",
    "print('Time Elapsed: {0:.2f}s'.format(\n",
    "        time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot pass the label \"Credit Analyst\" or \"Financial Examiner\" into the classifier. Instead we to encode them as 0s and 1s using the labelencoder as a part of sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relabel our labels as a 0 or 1\n",
    "le = preprocessing.LabelEncoder() \n",
    "le.fit(labels)\n",
    "labels_binary = le.transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to create arrays of indices so we can access the training and testing sets accoringly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = df_train.shape[0]\n",
    "train_set_idx = np.arange(0,train_size)\n",
    "test_set_idx = np.arange(train_size, len(labels))\n",
    "train_labels_binary = labels_binary[train_set_idx]\n",
    "test_labels_binary = labels_binary[test_set_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier we are using in the example is LogisticRegression. As we saw in the Machine Learning tutorial we first decide on a classifier, then we fit the classfier to create a model. We can then test our model with our testing set by passing in the features of the testing set. The model will output the probablity of each document being classfied as a Credit Analyst or Financial Analyst. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty='l1')\n",
    "mdl = clf.fit(train_tfidf, labels_binary[train_set_idx]) #train the classifer to get the model\n",
    "y_score = mdl.predict_proba( test_tfidf ) #score of the document being an ad for Credit or Financial Analyst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_n(y_true, y_prob, model_name):\n",
    "    \"\"\"\n",
    "    y_true: ls\n",
    "        ls of ground truth labels\n",
    "    y_prob: ls\n",
    "        ls of predic proba from model\n",
    "    model_name: str\n",
    "        str of model name (e.g, LR_123)\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    y_score = y_prob\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true, y_score)\n",
    "    precision_curve = precision_curve[:-1]\n",
    "    recall_curve = recall_curve[:-1]\n",
    "    pct_above_per_thresh = []\n",
    "    number_scored = len(y_score)\n",
    "    for value in pr_thresholds:\n",
    "        num_above_thresh = len(y_score[y_score>=value])\n",
    "        pct_above_thresh = num_above_thresh / float(number_scored)\n",
    "        pct_above_per_thresh.append(pct_above_thresh)\n",
    "    pct_above_per_thresh = np.array(pct_above_per_thresh)\n",
    "    plt.clf()\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(pct_above_per_thresh, precision_curve, 'b')\n",
    "    ax1.set_xlabel('percent of population')\n",
    "    ax1.set_ylabel('precision', color='b')\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(pct_above_per_thresh, recall_curve, 'r')\n",
    "    ax2.set_ylabel('recall', color='r')\n",
    "    ax2.set_ylim(0,1.05)\n",
    "    \n",
    "    name = model_name\n",
    "    plt.title(name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " plot_precision_recall_n(labels_binary[test_set_idx], y_score[:,1], 'LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we examine our precision-recall curve we can see that our precision is 1 and recall is 0.8 up to 0.4 percent of the population. Unlike the previous example where we are using a precision at k curve to prioritize our resources. We can still use a precision at k curve to see what parts of the corpus can be tagged by the classifier and which should undergo a manual clerical review. Based on this we can make decisions of what documents should be manually tagged by a person during a clerical rewiew, say, the percent of the population above 40%. \n",
    "\n",
    "Alternatively, we can try to maximize the entire precision-recall space. In this case we need a different metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall(y_true,y_score):\n",
    "    \"\"\"\n",
    "    Plot a precision recall curve\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: ls\n",
    "        ground truth labels\n",
    "    y_score: ls\n",
    "        score output from model\n",
    "    \"\"\"\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true,y_score[:,1])\n",
    "    plt.plot(recall_curve, precision_curve)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    auc_val = auc(recall_curve,precision_curve)\n",
    "    print('AUC-PR: {0:1f}'.format(auc_val))\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall(labels_binary[test_set_idx],y_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the area under the curve, 0.96, we see we have a very good classifier. The AUC shows how accurate our scores are under different cut-off thresholds. If you recall from the Machine Learning tutorial, the model outputs a score. We then set a cutoff to bin each score as a 0 or 1. The closer our scores are to the true values the more resilent they are to different cutoffs. For instance, if our scores were perfect our AUC would be 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_feature_importances(coef,features, labels, num_features=10):\n",
    "    \"\"\"\n",
    "    output feature importances\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    coef: numpy\n",
    "        feature importances\n",
    "    features: ls \n",
    "        feature names\n",
    "    labels: ls\n",
    "        labels for the classifier\n",
    "    num_features: int\n",
    "        number of features to output (default 10)\n",
    "    \n",
    "    Example\n",
    "    --------\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    coef = mdl.coef_.ravel()\n",
    "\n",
    "    dict_feature_importances = dict( zip(features, coef) )\n",
    "    orddict_feature_importances = OrderedDict( \n",
    "                                    sorted(dict_feature_importances.items(), key=lambda x: x[1]) )\n",
    "\n",
    "    ls_sorted_features  = list(orddict_feature_importances.keys())\n",
    "\n",
    "    num_features = 10\n",
    "    label0_features = ls_sorted_features[:num_features] \n",
    "    label1_features = ls_sorted_features[-num_features:] \n",
    "\n",
    "    print(labels[0],label0_features)\n",
    "    print(labels[1], label1_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_feature_importances(mdl.coef_.ravel(), features, ['Credit Analysts','Financial Examiner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature importances are which words are the most relevant for predicting the type of Job Ad. We would expect words like credit, customer and candidate to be found in ads for a Credit Analyst. While words like review officer, compliance would be found in ads for a Financial Examiner. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "Recall from the machine learning tutorial that we are seeking the find the most general pattern in the data in order to have to most general model that will be successfull at classfying new unseen data. Our previous strategy above was the *Out-of-sample and holdout set*. With this strategy we try to find a general pattern by randomly dividing our data into a test and training set based on some percentage split (e.g., 50-50 or 80-20). We train on the test set and evalute on the test set, where we pretend the test set is unforseen data. A significant drawback with this approach is we may be lucky or unlucky with our random split. A possible solution is to create many random splits into training and testing sets and evaluate each split to estimate the performance of a given model. \n",
    "\n",
    "A more sophisticated holdout training and testing procedure is *cross-validation*. In cross-validation we split our data into k-folds or k-partions, usually 5 or 10 folds. We then iterate k times. In each iteration one of the folds is used as a testing set and the rest of the folds are combined to form the training set. We can then evaluate the performance at each iteration to estimate the performance of a given method. An advantage of using cross-validation is all examples of data are used in the training set at least once. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_train_bag_of_words(train_corpus, test_corpus):\n",
    "    \"\"\"\n",
    "    Create test and training set bag of words\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_corpus: ls\n",
    "        ls of raw text for text corpus.\n",
    "    test_corpus: ls\n",
    "        ls of raw text for train corpus. \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    (train_bag_of_words,test_bag_of_words): scipy sparse matrix\n",
    "        bag-of-words representation of train and test corpus\n",
    "    features: ls\n",
    "        ls of words used as features. \n",
    "    \"\"\"\n",
    "    #parameters for vectorizer \n",
    "    ANALYZER = \"word\" #unit of features are single words rather then phrases of words \n",
    "    STRIP_ACCENTS = 'unicode'\n",
    "    TOKENIZER = None\n",
    "    NGRAM_RANGE = (0,2) #Range for pharases of words\n",
    "    MIN_DF = 0.01 # Exclude words that have a frequency less than the threshold\n",
    "    MAX_DF = 0.8  # Exclude words that have a frequency greater then the threshold \n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer=ANALYZER,\n",
    "                                tokenizer=None, # alternatively tokenize_and_stem but it will be slower \n",
    "                                ngram_range=NGRAM_RANGE,\n",
    "                                stop_words = stopwords.words('english'),\n",
    "                                strip_accents=STRIP_ACCENTS,\n",
    "                                min_df = MIN_DF,\n",
    "                                max_df = MAX_DF)\n",
    "    \n",
    "    NORM = None #turn on normalization flag\n",
    "    SMOOTH_IDF = True #prvents division by zero errors\n",
    "    SUBLINEAR_IDF = True #replace TF with 1 + log(TF)\n",
    "    USE_IDF = True #flag to control whether to use TFIDF\n",
    "\n",
    "    transformer = TfidfTransformer(norm = NORM,smooth_idf = SMOOTH_IDF,sublinear_tf = True)\n",
    "\n",
    "    #get the bag-of-words from the vectorizer and\n",
    "    #then use TFIDF to limit the tokens found throughout the text \n",
    "    train_bag_of_words = vectorizer.fit_transform( train_corpus ) \n",
    "    test_bag_of_words = vectorizer.transform( test_corpus )\n",
    "    if USE_IDF:\n",
    "        train_tfidf = transformer.fit_transform(train_bag_of_words)\n",
    "        test_tfidf = transformer.transform(test_bag_of_words)\n",
    "    features = vectorizer.get_feature_names()\n",
    "\n",
    "    \n",
    "    return train_tfidf, test_tfidf, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "cv = StratifiedKFold(train_labels_binary, n_folds=5)\n",
    "train_labels_binary = le.transform(train_labels)\n",
    "for i, (train,test) in enumerate(cv):\n",
    "    cv_train = train_corpus[train]\n",
    "    cv_test = train_corpus[test]\n",
    "    bag_of_words_train, bag_of_words_test, feature_names = create_test_train_bag_of_words(cv_train, \n",
    "                                                                                          cv_test)\n",
    "    \n",
    "    probas_ = clf.fit(bag_of_words_train, \n",
    "                      train_labels_binary[train]).predict_proba(bag_of_words_test)\n",
    "    cv_test_labels = train_labels_binary[test]\n",
    "    \n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(cv_test_labels,\n",
    "                                                                          probas_[:,1])\n",
    "    auc_val = auc(recall_curve,precision_curve)\n",
    "    plt.plot(recall_curve, precision_curve, label='AUC-PR {0} {1:.2f}'.format(i,auc_val))\n",
    "    \n",
    "plt.ylim(0,1.05)    \n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend(loc=\"lower left\", fontsize='x-small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we did 5-fold cross-validation and plotted precision-recall curves for each iteration. We can then average the AUC-PR of each iteration to estimate the performance of our method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Examples of tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_comments = 2\n",
    "label0_comment_idx = y_score[:,1].argsort()[:num_comments] #SuicideWatch\n",
    "label1_comment_idx = y_score[:,1].argsort()[-num_comments:] #depression\n",
    "test_set_labels = labels[test_set_idx]\n",
    "#convert back to the indices of the original dataset\n",
    "top_comments_testing_set_idx = np.concatenate([label0_comment_idx, \n",
    "                                               label1_comment_idx])\n",
    "\n",
    "\n",
    "#these are the 5 comments the model is most sure of \n",
    "for i in top_comments_testing_set_idx:\n",
    "    print(\n",
    "        u\"\"\"{}:{}\\n---\\n{}\\n===\"\"\".format(test_set_labels[i],\n",
    "                                          y_score[i,1],\n",
    "                                          test_corpus[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the top-2 example for each label that the model is sure of. In this case we can see our important feature words in the ads and see how the model classified these advertisements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Further Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A great resource for NLP in python is \n",
    "[Natural Language Processing with Python](https://www.amazon.com/Natural-Language-Processing-Python-Analyzing/dp/0596516495)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work thorugh the Reddit_TextAnalysis.ipynb notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
