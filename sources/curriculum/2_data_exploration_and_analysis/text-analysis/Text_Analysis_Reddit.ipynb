{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Text Analysis is used for summarizing or getting useful information out of a large amount of unstructured text stored in documents. This opens up the oppurtunity of using text data alongside more conventional data sources (e.g, surveys and administrative data). The goal of text analysis is to take a large corpus of complex and unstructured text data and extract important and meaningful messages in a comprehensible meaningful way. \n",
    "\n",
    "Text Analysis can help with the following tasks:\n",
    "\n",
    "* **Informationa retrieval**: Help find relevant information in large databases such as a systematic literature review. \n",
    "\n",
    "* **Clustering and text categorization**: Techniques like topic modeling modeling can summarize a large corpus of text by finding the most important phrases. \n",
    "\n",
    "* **Text Summarization**: Create category-sensitive text summaries of a large corpus of text. \n",
    "\n",
    "* **Machine Translation**: Translate from one language to another. \n",
    "\n",
    "In this tutorial we are going to analyze reddit posts from May 2015 in order to classify which subreddit a post origniated from and also do topic modelling to categorize posts into topcs made up of co-ocurring words. \n",
    "\n",
    "\n",
    "## Glossary of Terms\n",
    "\n",
    "* **Tokenize**: Tokenization is the process by which text is sepearated into meaningful terms or phrases. In english this is fairly triial as words as separated by whitespace. \n",
    "\n",
    "* **Stemming**: Stemming is a type of text normalization where words that have different forms but their essential meaning are normalized to the original dictionary form of a word. For example \"go,\" \"went,\" and \"goes\" all stem from the lemma \"go.\"\n",
    "\n",
    "* **TFIDF**: TFIDF (Term frequency-inverse document frequency) is an example of feature enginnering where the most important words are extracted by taking account their frequency in documents and the entire corpus of documents as a whole.\n",
    "\n",
    "* **Topic Modeling**: Topic modeling is an unsupervised learning method where groups of co-occuring words are clustered into topics. Typically, the words in a a cluster should be related and make sense (e.g, boat, ship, captain). Individual documents will then fall into multiple topics. \n",
    "\n",
    "* **LDA**: LDA (latent Dirichlet allocation) is a type of probabilistic model commonly used for topic modelling. \n",
    "\n",
    "* **Stop Words**: Stop words are words that have little semantic meaning like prepositions, articles and common nouns. They can often be ingnored. \n",
    "\n",
    "\n",
    "## Table of Contents\n",
    " - [Data Source](#Data-Source:-Reddit-Comments-from-May-2015-in-JSON-format)\n",
    " - [Preprocess](#Preprocess-the-data)\n",
    " - [Supervised Learning](#Supervised-Learning:-Identify-the-Subreddit-Section)\n",
    " - [Unsupervised Learning](#Topic-Modeling:-Unsupervised-Learning )\n",
    " \n",
    " ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "import matplotlib.pyplot\n",
    "import nltk\n",
    "import ujson\n",
    "import re\n",
    "import time\n",
    "\n",
    "from __future__ import print_function\n",
    "from six.moves import zip, range \n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter, OrderedDict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer\n",
    "\n",
    "%matplotlib inline\n",
    "nltk.download('stopwords') #download the latest stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reddit(fname, ls_subreddits=[], MIN_CHAR=30):\n",
    "    \"\"\"\n",
    "    Loads Reddit Comments from a json file based on \n",
    "    whether they are in the selected subreddits and \n",
    "    have more than the MIN_CHARACTERS\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fname: str\n",
    "        filename\n",
    "    ls_subreddits: ls[str]\n",
    "        list of subreddits to select from \n",
    "    MIN_CHAR: int\n",
    "        minimum number of characters necessary to select\n",
    "        a comment\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    corpus: ls[str]\n",
    "        list of selected reddit comments\n",
    "    subreddit_id: array[int]\n",
    "        np.array of indices that match with the ls_subreddit\n",
    "        index \n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    subreddit_id = []\n",
    "    with open(fname, 'r') as infile:\n",
    "        for line in infile:\n",
    "            dict_reddit_post =  ujson.loads(line)\n",
    "            subreddit = dict_reddit_post['subreddit']\n",
    "            n_characters = len( dict_reddit_post['body'] )\n",
    "            \n",
    "            if ls_subreddits: #check that the list is not empty\n",
    "                in_ls_subreddits = subreddit in ls_subreddits\n",
    "            else:\n",
    "                in_ls_subreddits = True\n",
    "            \n",
    "            grter_than_min = n_characters > MIN_CHAR\n",
    "            \n",
    "            if ( grter_than_min and in_ls_subreddits ):\n",
    "                corpus.append(dict_reddit_post['body'])\n",
    "                subreddit_id.append(subreddit)\n",
    "                \n",
    "    return np.array(corpus), np.array(subreddit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_n(y_true, y_prob, model_name, fname=None):\n",
    "    \"\"\"\n",
    "    create a precision recall curve\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: ls\n",
    "        ls of ground truth labels \n",
    "    y_prob: ls\n",
    "        ls of predict probas from model\n",
    "    model_name: str\n",
    "        str of model used\n",
    "    fname: str\n",
    "        filename to save figure\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Plot of precision recall. \n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    y_score = y_prob\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true, y_score)\n",
    "    precision_curve = precision_curve[:-1] #take every value up to the last one \n",
    "    recall_curve = recall_curve[:-1]# take every value up to the last one\n",
    "    pct_above_per_thresh = []\n",
    "    number_scored = len(y_score)\n",
    "    for value in pr_thresholds:\n",
    "        num_above_thresh = len(y_score[y_score>=value])\n",
    "        pct_above_thresh = num_above_thresh / float(number_scored)\n",
    "        pct_above_per_thresh.append(pct_above_thresh)\n",
    "    pct_above_per_thresh = np.array(pct_above_per_thresh)\n",
    "    plt.clf()\n",
    "    \n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(pct_above_per_thresh, precision_curve, 'b')\n",
    "    ax1.set_xlabel('percent of population')\n",
    "    ax1.set_ylabel('precision', color='b')\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(pct_above_per_thresh, recall_curve, 'r')\n",
    "    ax2.set_ylabel('recall', color='r')\n",
    "    \n",
    "    name = model_name\n",
    "    plt.title(name)\n",
    "    if(fname):\n",
    "        plt.savefig(fname)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "### Data Source: Reddit Comments from May 2015 in JSON format\n",
    "\n",
    "For the superivised learning portion of the tutorial we will being attempting to classify whether reddit threads have come from the SucideWatch or depression. These two threads should be somewhat similiar so it poses a non-trivial challenge for a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash #magic function to run a bash command inside of a Jupyter notebook. \n",
    "#unizip the data\n",
    "gunzip ./data/RC_2015-05.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab data from the following subreddits\n",
    "ls_subreddits = ['SuicideWatch', 'depression']\n",
    "[corpus, subreddit_id] = load_reddit('./data/RC_2015-05.json', ls_subreddits, MIN_CHAR=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matches are non-word chracters and digits to be replaced with spaces.\n",
    "RE_PREPROCESS = r'\\W+|\\d+'  \n",
    "#get rid of punctuation and make everything lowercase\n",
    "processed_corpus = np.array( [ re.sub(RE_PREPROCESS, ' ', comment).lower() for comment in corpus] )\n",
    "Counter(subreddit_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning: Identify the Subreddit Section\n",
    "\n",
    "In this section we are going to train a classifier to properly tag the original subreddit the comment appeared. First we split our data into a testing and training set using the first 80% of the data as the training set and the remaining 20% as the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into training and testing sets. \n",
    "#refactor this in the test train-split\n",
    "train_set_size = int(0.8*len(subreddit_id))\n",
    "train_idx = np.arange(0,train_set_size)\n",
    "test_idx = np.arange(train_set_size, len(subreddit_id))\n",
    "\n",
    "train_subreddit_id = subreddit_id[train_idx]\n",
    "train_corpus = processed_corpus[train_idx]\n",
    "\n",
    "test_subreddit_id = subreddit_id[test_idx]\n",
    "test_corpus = processed_corpus[test_idx]\n",
    "\n",
    "print('Training Labels', Counter(subreddit_id[train_idx]))\n",
    "print('Testing Labels', Counter((subreddit_id[test_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize and stem to create features\n",
    "\n",
    "Now that we have the data and we have done a bit of preprocessing. We want to create features. Now we create a vectorizer object that finds the frequency of words in each of the documents while weighing the importance of each word. For example, the words *the* or *for* may appear often in a document but may have very little semantic value. Conversely a document may have specialized, obscure words that do not occur anywhere in else in the corpus. These cases are managed by setting a threshold for the Min and Max Document Frequeny(DF).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for vectorizer \n",
    "ANALYZER = \"word\" #unit of features are single words rather then phrases of words \n",
    "STRIP_ACCENTS = 'unicode'\n",
    "TOKENIZER = None\n",
    "NGRAM_RANGE = (0,2) #Range for pharases of words\n",
    "MIN_DF = 0.01 # Exclude words that have a frequency less than the threshold\n",
    "MAX_DF = 0.8  # Exclude words that have a frequency greater then the threshold \n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=ANALYZER,\n",
    "                            tokenizer=None, # alternatively tokenize_and_stem but it will be slower \n",
    "                            ngram_range=NGRAM_RANGE,\n",
    "                            stop_words = stopwords.words('english'),\n",
    "                            strip_accents=STRIP_ACCENTS,\n",
    "                            min_df = MIN_DF,\n",
    "                            max_df = MAX_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF (Term Frequency Inverse Document Frequency) transforms a count matrix--what we created above--into a TFIDF represenation. This is done by reweigthing words that occur throughout the entire corpus to a lower weight due to empirically being found to be less informative. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORM = None #turn on normalization flag\n",
    "SMOOTH_IDF = True #prvents division by zero errors\n",
    "SUBLINEAR_IDF = True #replace TF with 1 + log(TF)\n",
    "USE_IDF = True #flag to control whether to use TFIDF\n",
    "\n",
    "transformer = TfidfTransformer(norm = NORM,smooth_idf = SMOOTH_IDF,sublinear_tf = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the bag-of-words from the vectorizer and\n",
    "#then use TFIDF to limit the tokens found throughout the text \n",
    "start_time = time.time()\n",
    "train_bag_of_words = vectorizer.fit_transform( train_corpus ) #using all the data on for generating features!! Bad!\n",
    "test_bag_of_words = vectorizer.transform( test_corpus )\n",
    "if USE_IDF:\n",
    "    train_tfidf = transformer.fit_transform(train_bag_of_words)\n",
    "    test_tfidf = transformer.transform(test_bag_of_words)\n",
    "features = vectorizer.get_feature_names()\n",
    "print('Time Elapsed: {0:.2f}s'.format(\n",
    "        time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relabel our labels as a 0 or 1\n",
    "le = preprocessing.LabelEncoder() \n",
    "le.fit(subreddit_id)\n",
    "subreddit_id_binary = le.transform(subreddit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make clear what are the features and what are the labels \n",
    "clf = LogisticRegression(penalty='l1')\n",
    "mdl = clf.fit(train_tfidf, \n",
    "              subreddit_id_binary[train_idx])\n",
    "y_score = mdl.predict_proba( test_tfidf )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalution of the Supervised Model\n",
    "\n",
    "To evalute how are classifer had done we find the AUC (Area Under Curve) of a ROC Curve and plot a precision recall curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score( subreddit_id_binary[test_idx], y_score[:,1])\n",
    "print(\"{0:.2f}\".format(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " plot_precision_recall_n(subreddit_id_binary[test_idx], y_score[:,1], 'LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Feature Importances \n",
    "\n",
    "Find the five words that are most predictive of each subreddit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = mdl.coef_.ravel()\n",
    "\n",
    "dict_feature_importances = dict( zip(features, coef) )\n",
    "orddict_feature_importances = OrderedDict( \n",
    "                                sorted(dict_feature_importances.items(), key=lambda x: x[1]) )\n",
    "\n",
    "ls_sorted_features  = list(orddict_feature_importances.keys())\n",
    "\n",
    "num_features = 5\n",
    "subreddit0_features = ls_sorted_features[:5] #SuicideWatch\n",
    "subreddit1_features = ls_sorted_features[-5:] #depression\n",
    "print('SuicideWatch: ',subreddit0_features)\n",
    "print('depression: ', subreddit1_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See the predictions and how well they match up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maybe do something with this crazy indexing: this is python not C!\n",
    "num_comments = 5\n",
    "subreddit0_comment_idx = y_score[:,1].argsort()[:num_comments] #SuicideWatch\n",
    "subreddit1_comment_idx = y_score[:,1].argsort()[-num_comments:] #depression\n",
    "\n",
    "#convert back to the indices of the original dataset\n",
    "top_comments_testing_set_idx = np.concatenate([subreddit0_comment_idx, \n",
    "                                               subreddit1_comment_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are the 5 comments the model is most sure of \n",
    "for i in top_comments_testing_set_idx:\n",
    "    print(\n",
    "        u\"\"\"{}:{}\\n---\\n{}\\n===\"\"\".format(test_subreddit_id[i],\n",
    "                                          y_score[i,1],\n",
    "                                          test_corpus[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The predict probability refer to the probablity of a comment belonging to the depression subreddit. Therefore, comments belonging to the SucideWatch subreddit will have a low probablity. As can be seen from the comments for the three highest probablities for SucideWatch and depression, the classifier does a good job. Note the last entry in the depression comment is miscategorized. This is most likely due to the comment referring to depression and treatment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling: Unsupervised Learning\n",
    "\n",
    "In this portion of the tutorial we will be extracting topics in the form of commonly co-occuring words from the corpus of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOPICS = 50\n",
    "N_TOP_WORDS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load All the reddit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "corpus_all, subreddit_id_all = load_reddit('./data/RC_2015-05.json',MIN_CHAR=250)\n",
    "end = time.time()\n",
    "print('Loading takes {0:2f}s'.format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of punctuation and set to lowercase  \n",
    "start = time.time()\n",
    "processed_corpus_all = [ re.sub( RE_PREPROCESS, ' ', comment).lower() for comment in corpus_all]\n",
    "\n",
    "#tokenzie the words\n",
    "bag_of_words_all = vectorizer.fit_transform( processed_corpus_all ) \n",
    "end = time.time() \n",
    "#grab the features/vocabulary\n",
    "features_all = vectorizer.get_feature_names()\n",
    "print(\"Processing took {}s\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Counter(subreddit_id_all), len(subreddit_id_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create topics\n",
    "\n",
    "To create our topics we will use the LatentDirichletAllocation algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lda = LatentDirichletAllocation( n_topics = N_TOPICS )\n",
    "doctopic = lda.fit_transform( bag_of_words_all )\n",
    "end = time.time() \n",
    "print(\"Processing took {}s\".format(end- start)) # takes ~72s for 1 file, ~445s for 5 files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the top ten words for each topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ls_keywords = []\n",
    "for i,topic in enumerate(lda.components_):\n",
    "    word_idx = np.argsort(topic)[::-1][:N_TOP_WORDS]\n",
    "    keywords = ', '.join( features_all[i] for i in word_idx)\n",
    "    ls_keywords.append(keywords)\n",
    "    print(i, keywords)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first 25 comments with the majority topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_comments = 50\n",
    "for comment_id in range(num_comments):\n",
    "    topic_id = np.argsort(doctopic[comment_id])[::-1][0]\n",
    "    print('comment_id:',\n",
    "          comment_id,\n",
    "          subreddit_id_all[comment_id],\n",
    "          'topic_id:',\n",
    "          topic_id,\n",
    "          'keywords:',\n",
    "           ls_keywords[topic_id])\n",
    "    print('---')\n",
    "    print(corpus_all[comment_id])\n",
    "    print('===')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return to [TOC](#Table-of-Contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
