@article{Tibshirani2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1304.2986v2},
author = {Tibshirani, Ryan J.},
doi = {10.1214/13-AOS1189},
eprint = {arXiv:1304.2986v2},
file = {:home/jeremy/Downloads/trendfilter.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Lasso stability,Locally adaptive regression splines,Minimax convergence rate,Nonparametric regression,Smoothing splines,Trend filtering},
number = {1},
pages = {285--323},
title = {{Adaptive piecewise polynomial estimation via trend filtering}},
volume = {42},
year = {2014}
}


@article{Gray2003,
author = {Gray, Alexander G and Moore, Andrew W and Inst, Robotics},
file = {:home/jeremy/Downloads/1.9781611972733.19.pdf:pdf},
keywords = {algorithms,divide-and-conquer,estimating a probability density,from data is a,fundamental,kernel density estimation,native methods such as,nonparametric statistics,space-partitioning trees,support vector machines,the task of},
pages = {203--211},
title = {{Nonparametric Density Estimation : Toward Computational Tractability}},
year = {2003}
}


@article{Szegedy2014,
archivePrefix = {arXiv},
arxivId = {1312.6199v4},
author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian J. and Fergus, Rob},
eprint = {1312.6199v4},
file = {:home/jeremy/Downloads/1312.6199.pdf:pdf},
journal = {arXiv preprint},
pages = {1--10},
title = {{Intriguing properties of neural networks, Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus}},
url = {https://arxiv.org/pdf/1312.6199v4.pdf},
year = {2014}
}
@article{Mukherjee2006,
abstract = {version corrects some of the remaining typos and imprecisions.},
author = {Mukherjee, Sayan and Niyogi, Partha and Poggio, Tomaso and Rifkin, Ryan},
doi = {10.1007/s10444-004-7634-z},
file = {:home/jeremy/Downloads/Mukherjee2006{\_}Article{\_}LearningTheoryStabilityIsSuffi.pdf:pdf},
isbn = {1019-7168},
issn = {10197168},
journal = {Advances in Computational Mathematics},
keywords = {Consistency,Empirical risk minimization,Generalization,Inverse problems,Stability,Uniform Glivenko-Cantelli},
number = {1-3},
pages = {161--193},
title = {{Learning theory: Stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization}},
volume = {25},
year = {2006}
}
@article{Xu2012,
abstract = {We derive generalization bounds for learning algorithms based on their robustness: the property that if a testing sample is "similar" to a training sample, then the testing error is close to the training error. This provides a novel approach, different from the complexity or stability arguments, to study generalization of learning algorithms. We further show that a weak notion of robustness is both sufficient and necessary for generalizability, which implies that robustness is a fundamental property for learning algorithms to work.},
archivePrefix = {arXiv},
arxivId = {1005.2243},
author = {Xu, Huan and Mannor, Shie},
doi = {10.1007/s10994-011-5268-1},
eprint = {1005.2243},
file = {:home/jeremy/Downloads/1005.2243.pdf:pdf},
isbn = {9780982252925},
issn = {08856125},
journal = {Machine Learning},
keywords = {Generalization,Non-IID sample,Quantile loss,Robust},
number = {3},
pages = {391--423},
pmid = {19626713},
title = {{Robustness and generalization}},
volume = {86},
year = {2012}
}
@article{Radivojac2004,
abstract = {We investigate the problem of supervised feature selection within the filtering framework. In our approach, applicable to the two-class problems, the feature strength is inversely proportional to the p-value of the null hypothesis that its class-conditional densities, p(X | Y = 0) and p(X | Y = 1), are identical. To estimate the p-values, we use Fisher's permutation test combined with the four simple filtering criteria in the roles of test statistics: sample mean difference, symmetric Kullback-Leibler distance, information gain, and chi-square statistic. The experimental results of our study, performed using naive Bayes classifier and support vector machines, strongly indicate that the permutation test im-proves the above-mentioned filters and can be used effectively when sample size is relatively small and number of features relatively large.},
author = {Radivojac, P. and Obradovic, Z. and Dunker, A. K. and Vucetic, S.},
doi = {10.1007/978-3-540-30115-8_32},
file = {:home/jeremy/Downloads/radivojac{\_}ecml04.pdf:pdf},
isbn = {3-540-23105-6},
issn = {03029743},
journal = {Machine Learning: Ecml 2004, Proceedings},
pages = {334--346},
title = {{Feature selection filters based on the permutation test}},
volume = {3201},
year = {2004}
}
@article{Kruskal,
abstract = {Ordinally invariant, i.e., rank, measures of association for bivariate populations are discussed, with emphasis on the probabilistic and operational interpretations of their population values. The three measures considered at length are the quadrant measure, Kendall's tau, and Spearman's rho. Relationships between these measures are discussed, as are connections between these measures and certain measures of association for cross classifications. Sampling theory is surveyed with special attention to the motivation for sample values of the measures. The historical development of ordinal measures of association is outlined.},
author = {Kruskal, William H},
issn = {01621459},
journal = {Journal of the American Statistical Association},
number = {284},
pages = {814--861},
publisher = {[American Statistical Association, Taylor {\&} Francis, Ltd.]},
title = {{Ordinal Measures of Association}},
url = {http://www.jstor.org/stable/2281954},
volume = {53},
year = {1958}
}
@article{Bogachev,
author = {Bogachev, Vladimir I and Kolesnikov, Aleksandr V},
journal = {Russian Mathematical Surveys},
number = {5},
pages = {785},
title = {{The Monge-Kantorovich problem: achievements, connections, and perspectives}},
url = {http://stacks.iop.org/0036-0279/67/i=5/a=R01},
volume = {67},
year = {2012}
}
@article{Goodfellow2014,
abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
archivePrefix = {arXiv},
arxivId = {1412.6572},
author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1412.6572},
file = {:home/jeremy/Downloads/1412.6572.pdf:pdf},
isbn = {1412.6572},
issn = {0012-7183},
pages = {1--11},
pmid = {729514},
title = {{Explaining and Harnessing Adversarial Examples}},
url = {http://arxiv.org/abs/1412.6572},
year = {2014}
}
